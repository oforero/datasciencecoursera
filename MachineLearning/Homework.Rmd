---
title: "Coursera - Practical Machine Learning - Project"
author: "Oscar Forero"
date: "21 Aug 2014"
output: html_document
---

# Practical Machine Learning - Homework

## Description

Train a machine learning model using the following data set:
[Training](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

Then use the model to predict the class of the following data set:
[Testing](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The data has information from sensors used by people performing a physical exercise, each row is label as correctly performed 'A' or incorrectly perform 'B', 'C', 'D', 'E', the name of the column is 'classe'. 
The model should predict the â€˜classe' variable, there is no specific requirement about which other fields to use or not to use.

## Setting up the environment

Import required libraries and set the working directory, you will have to change this if you want to execute the document.

I also use the doMC library to run caret in multiple cores.

```{r, results='hide', message=FALSE}
library(utils)
library(caret)
library(ggplot2)
library(doMC)

setwd("/Users/Oscar/Classes/JohnsHopkins/datasciencecoursera/MachineLearning")
registerDoMC(cores = 4)

```

## Download the data files

The following chunk of code should download
```{r, eval=FALSE}
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "pml-training.csv", method="internal")

download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "pml-testing.csv", method="internal")

```

## Loading and cleaning the data, splitting into training and testing data sets

First load the data treating spaces, blanks, divisions by 0 and NA as not a numeric value.
After looking at the data it is clear that many columns are sparsely populated, because there are many other fully populated fields I decided to remove all columns with NA values.

A review of the data shows that some fields are not lectures from the sensors but labeling information and of little relevance for the model creation. For this reason I removed the columns "X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window" 

As a final step used the cleaned data set to create a training and a testing data set.

```{r}
inputDS <- read.csv("pml-training.csv", na.strings=c("", "NA", "#DIV/0!"))
naColumns <- apply(inputDS, 2, function(x) any(is.na(x)))

completeDS <- inputDS[,!naColumns]
remove <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")

cleanDS <- completeDS[, !names(completeDS) %in% remove]

split <- createDataPartition(cleanDS$classe, p=0.7, list=FALSE)
training <- cleanDS[split,]
testing <- cleanDS[-split,]

summary(training)
```

The predictors left to create the model are:

```{r, echo=FALSE}
featurePlot(x=training[,! names(training) %in% c("classe")], y=training$classe)
```

## A basic model
I decided to use a decision tree model as a benchmark to compare other models of higher complexity.

```{r}
set.seed(6996)

dtModel <- train(classe ~ ., data=training, method="rpart")
dtModel$results
confusionMatrix(dtModel)
```


## Training the model

Next step is to train a model using cross validation to choose the best parameters. 
I decided to try a random forest model first because in my experience it yields good results. 

```{r}
set.seed(90210)

fitControl <- trainControl(method="cv", number=5, repeats=5, classProbs=TRUE)
rfModel <- train(classe ~ ., data=training, method="rf", trControl=fitControl)

rfModel$results
confusionMatrix(rfModel)

```

## Estimated Cross-Validation Error 

```{r}
rfFinal <- rfModel$finalModel
rfFinal

rfOOB <- round(rfFinal$err.rate[rfFinal$ntree, "OOB"]*100, digits=2)
```
The estimated out of bag error from the model as calculated by CARET is `r rfOOB`%. 
It can be found in the finalModel, to extracted I used the formula above which was extracted from the R source code by an anonymous student.

## Error estimation in the Testing Set 

```{r}
pred <- predict(rfModel, newdata=testing)
confMatrix <- confusionMatrix(testing$classe, pred)

accuracy <- confMatrix$overall[1]
error <- round((1 - accuracy)*100, digits=2)

confMatrix
```

In the lectures the recommendation is to still use a testing set that had not been used to train the model.
The error is calculated by subtracting the Accuracy from 1, and in this case is `r error`%


## Conclussion

The Random Forest model takes more time to train than the simple tree model but it has higher accuracy. 
Given that it achieved `r accuracy`% accuracy, Because of that I saw no need to try higher complexity models.

The last step is to use the model to predict the labels for Homework evaluation data set:

```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

evalDS <- read.csv("pml-testing.csv")
pred <- predict(rfModel, newdata=evalDS)

pml_write_files(pred)
```

